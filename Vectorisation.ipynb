{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "celtic-whale",
   "metadata": {},
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/fr/thumb/e/ed/Logo_Universit%C3%A9_du_Maine.svg/1280px-Logo_Universit%C3%A9_du_Maine.svg.png' width=\"300\" height=\"500\">\n",
    "<br>\n",
    "<div style=\"border: solid 3px #000;\">\n",
    "    <h1 style=\"text-align: center; color:#000; font-family:Georgia; font-size:26px;\">Infrastructures pour l'IA</h1>\n",
    "    <p style='text-align: center;'>Master Informatique 1</p>\n",
    "    <p style='text-align: center;'>Anhony Larcher</p>\n",
    "</div>\n",
    "\n",
    "Cette session est organisée comme un challenge:\n",
    "* Vous optimiserez un code afin d'effectuer un calcul le plus rapidement possible\n",
    "* le résultat sera utilisée dans le TP numéro 6 pour faire du calcul parallélisé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f937f-8bd2-40e8-8ed5-1d35c51837b6",
   "metadata": {},
   "source": [
    "# Implémenter le calcul suivant et optimisez le au mieux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170d483",
   "metadata": {},
   "source": [
    "Il s'agit d'accumuler les statistiques d'ordre 0 et d'ordre 1 sur un mélange de Gaussiennes. Vous trouverez sur Umtice une archive zip contenant un objet Mixture et des paramètres acoustiques de type MFCC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3f254-3206-4924-985d-7ad095b801e7",
   "metadata": {},
   "source": "Chaque distribution de la mixture a pour densité de probabilité:"
  },
  {
   "cell_type": "markdown",
   "id": "5446f09b-e1b0-48cf-be6f-c6ab532bd8ac",
   "metadata": {},
   "source": [
    "Et la densité de probabilité du mélange de Gaussienne est \n",
    "$$p(x|\\lambda) = \\Sigma_{i=1}^{M}w_ip_i(x)$$\n",
    "\n",
    "où $$\\Sigma_{i=1}^{M}w_i = 1$$\n",
    "\n",
    "et \n",
    "\n",
    "$$p_i(x) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\Sigma_i|^\\frac{1}{2}} e^{-(\\frac{1}{2}(x - \\mu)^T\\Sigma_i^{-1}(x-\\mu))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-weather",
   "metadata": {},
   "source": [
    "Nous souhaiton ici calculer pour chaque trame $x_t$\n",
    "\n",
    "$$P(i|x_t) = \\frac{w_ip_i(x_t)}{\\Sigma_{j=1}^M w_j p_j(x_t)}$$\n",
    "\n",
    "Pour ensuite calculer les statistiques d'ordre 0 pour chaque gaussienne $i$:\n",
    "\n",
    "$$n_i = \\Sigma_{t=1}^{T}P(i|x_t)$$\n",
    "\n",
    "Les statistiques d'ordre 1:\n",
    "\n",
    "$$E_i(x) = \\frac{1}{n_i} \\Sigma_{t=1}^{T}P(i|x_t)x_t$$\n",
    "\n",
    "Et les statistiques d'ordre 2:\n",
    "\n",
    "$$E_i(x^2) = \\frac{1}{n_i} \\Sigma_{t=1}^{T}P(i|x_t)x_t^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03927e-a2e2-435b-afed-195b98682349",
   "metadata": {},
   "source": [
    "Les données sont disponibles ici:\n",
    "\n",
    "    https://umbox.univ-lemans.fr/index.php/s/CiP47cfw8NBMM5J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094eb943",
   "metadata": {},
   "source": [
    "## 1.1) Initialisez l'accumulateur de statistiques\n",
    "\n",
    "Le fichier ``mixture.py``contient le code d'une classe Mixtuyre qui sera utilisé dans ce TP. Ce code se trouve également dans la cellule ci-dessous pour travailler dans le notebook. Une fois la cellule ci-dessous complétée vous pourrez recopier le code dans le fichier ``mixture.py``afin d'avoir une version propre du code."
   ]
  },
  {
   "cell_type": "code",
   "id": "4099388c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T17:09:58.722639Z",
     "start_time": "2024-09-12T17:09:58.291132Z"
    }
   },
   "source": [
    "import numpy\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "\n",
    "class Mixture(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize an empty Mixture object\n",
    "        \"\"\"\n",
    "        self.w = numpy.array([])\n",
    "        self.mu = numpy.array([])\n",
    "        self.invcov = numpy.array([])\n",
    "        self.cst = numpy.array([])\n",
    "        self.det = numpy.array([])\n",
    "        self.D = 0 \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Serialize a Mixture object to text\n",
    "        \"\"\"\n",
    "        return f'w = {self.w.shape}{self.w}\\nmu = {self.mu.shape}{self.mu}\\ninvcov = {self.invcov.shape}{self.invcov}\\ncst = {self.cst.shape}{self.cst}\\ndet = {self.det.shape}{self.det}'\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, filename):\n",
    "        \"\"\"\n",
    "        Read a Mixture object stored in Pickle format on disk\n",
    "        :param filename: the name of the file to read from\n",
    "        :return: a Mixture object\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as fh:\n",
    "            mixture = pickle.load(fh)\n",
    "            return mixture\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Save a Mixture object to disk in Pickle format\n",
    "        :param filename: the name of the file to write to\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as fh:\n",
    "            pickle.dump(self, fh)\n",
    "\n",
    "    def loadPresets(self):\n",
    "        self.mu = numpy.load(\"mu.npy\")\n",
    "        self.w  = numpy.load(\"w.npy\")\n",
    "        self.invcov = numpy.load(\"invcov.npy\")\n",
    "        self.D = self.mu.shape[1]\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "83e53141",
   "metadata": {},
   "source": [
    "## Analyse des formules\n",
    "\n",
    "On remarque d'abord que certains éléments dépendent des données, et plus particulièrement de chaque vecteur de donnée, mais également de chaque distribution de la mixture.\n",
    "\n",
    "Nous verrons donc apparaitre 2 boucles principales:\n",
    "* une sur les données\n",
    "* une sur les distributions\n",
    "\n",
    "Pour simplifier le calcul nous devons donc séparer ce qui est indépendant de ces boucles afin de ne pas le recalculer plusieurs fois.\n",
    "\n",
    "Ré-écrivez $$p_i(x)$$ en séparant ces termes.\n",
    "\n",
    "$$p_i(x) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\Sigma_i|^\\frac{1}{2}} e^{-(\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i))}$$\n",
    "\n",
    "On trouve d'abord un terme qui dépend de chaque mixture: leur déterminant"
   ]
  },
  {
   "cell_type": "code",
   "id": "f17f0d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:34:22.161898Z",
     "start_time": "2024-09-12T19:34:22.069746Z"
    }
   },
   "source": [
    "# Il faut donc calculer ce déterminant une seule fois:\n",
    "# Les covariances inverses étant stockées par ligne dans self.invcov nous calculons simplement le produit de chaque ligne.\n",
    "# Le grand Sigma dans notre équation, c'est la matrice de covariance.\n",
    "# Comme on l'a expliqué en TD, pour simplifier les calculs on ne prend qu'une matrice diagonale, car ça va plus vite à calculer\n",
    "# Ce qui est chouette c'est que le determinant d'une matrice diagonale est juste le produit de ses termes diagonaux.\n",
    "# Seul petit détail, on nous a donné l'inverse de la covariance. Il faudra donc la recalculer.\n",
    "# C'est pas bien gênant, on fait juste 1/v pour chaque valeur, car dans le cas d'une matrice diagonale on peut calculer son inverse comme ça.\n",
    "#\n",
    "# Just a little note, in the following code cells of this notebook, I am not going to use the mixture object above, but rather do the code inline and adapt it in the Mixture class.\n",
    "\n",
    "from time import perf_counter_ns as perf\n",
    "\n",
    "invcov = numpy.load(\"invcov.npy\")\n",
    "\n",
    "# With loops and no numpy\n",
    "start = perf()\n",
    "det2 = []\n",
    "for line in invcov:\n",
    "    det2.append(1)\n",
    "    for val in line:\n",
    "        det2[-1] *= val\n",
    "    det2[-1] = 1 / det2[-1]\n",
    "end = perf()\n",
    "naive_imple = end-start\n",
    "       \n",
    "# With numpy and no loops \n",
    "start = perf()\n",
    "det = 1.0 / numpy.prod(invcov, axis=1)\n",
    "end = perf()\n",
    "numpy_imple = end-start\n",
    "\n",
    "print(f\"Naive implementation took {naive_imple/1000} microseconds\\n\"\n",
    "      f\"Numpy implementation took {numpy_imple/1000} microseconds\\n\"\n",
    "      f\"So, numpy here is {naive_imple/numpy_imple:.2f} times faster\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation took 2056.4 microseconds\n",
      "Numpy implementation took 265.3 microseconds\n",
      "So, numpy here is 7.75 times faster\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "a673e834",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant calculer le premier terme pour chaque distribution:\n",
    "    \n",
    "$$ \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\Sigma_i|^\\frac{1}{2}} $$ "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:36:36.179836Z",
     "start_time": "2024-09-12T19:36:36.170133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# D étant constant, on voit d'ores et déjà que (2Pi)^(D/2) est une constante également. Le déterminant de la matrice de covariance ne dépend pas des donnés, on peut donc calculer cette fraction en amont, une seule fois pour toutes les données puisqu'elles n'influent pas dans le calcul ici.\n",
    "# Enfin, n'oublions la racine carré sur le determinant de la covariance.\n",
    "# Manque plus qu'inverser tout ça et ce sera bon.\n",
    "\n",
    "import math\n",
    "\n",
    "D = invcov.shape[1]\n",
    "\n",
    "### Pure python implementation ###\n",
    "start = perf()\n",
    "cst2 = []\n",
    "for line in det:\n",
    "    cst2.append( 1.0 / ((2.0 * math.pi)**(D/2.0) * (math.sqrt(line))) )\n",
    "end = perf()\n",
    "naive_imple = end-start\n",
    "\n",
    "### Numpy implementation ###\n",
    "start = perf()\n",
    "cst = 1.0 / ( numpy.power(2.0 * numpy.pi, D/2.0) * numpy.sqrt( det ))\n",
    "end = perf()\n",
    "numpy_imple = end-start\n",
    "\n",
    "print(f\"Naive implementation took {naive_imple/1000} microseconds\\n\"\n",
    "      f\"Numpy implementation took {numpy_imple/1000} microseconds\\n\"\n",
    "      f\"So, numpy here is {naive_imple/numpy_imple:.2f} times faster\")\n"
   ],
   "id": "99a3e19e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation took 352.4 microseconds\n",
      "Numpy implementation took 194.2 microseconds\n",
      "So, numpy here is 1.81 times faster\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "a6c145a9",
   "metadata": {},
   "source": [
    "Regardons maintenant le terme contenu dans l'exponentielle:\n",
    "\n",
    "$$ -(\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i)) $$ \n",
    "\n",
    "Tout ce terme dépend des distributions, une partie seulement dépend des données.\n",
    "\n",
    "$$ -(\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i)) = -\\frac{1}{2} ( x^T\\Sigma_i^{-1}x + \\mu_i^T\\Sigma_i^{-1}\\mu_i - x^T\\Sigma_i^{-1}\\mu_i - \\mu_i^T\\Sigma_i^{-1}x ) $$ \n",
    "\n",
    "Calculez chacun de ces termes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528cade4",
   "metadata": {},
   "source": [
    "Analyse du terme independant des données:\n",
    "\n",
    "$$ \\mu_i^T\\Sigma_i^{-1}\\mu_i $$\n",
    "\n",
    "$$ \\mu_i^T$$ est de dimension (1, 39) $$\\Sigma_i$$ est de dimension (39, 39)\n",
    "\n",
    "Donc $$ \\mu_i^T\\Sigma_i^{-1}\\mu_i $$ est de dimension (1,)"
   ]
  },
  {
   "cell_type": "code",
   "id": "9756cb23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:36:40.110186Z",
     "start_time": "2024-09-12T19:36:40.090303Z"
    }
   },
   "source": [
    "# En pratique, comme la matrice de covariance est diagonale, nous n'avons pas besoin de calculer un vrai produit matriciel\n",
    "# Écrivez la valeur du premier terme de \\mu_i \\Sigma^{-1}_i et tirez partie de cette expression pour simplifier\n",
    "# le calcul de ce terme\n",
    "\n",
    "# self.mu est de dimension 64x39\n",
    "# self.invcov est de dimension 64x39\n",
    "# numpy.square(self.mu) * self.invcov) est de dimension 64x39\n",
    "# numpy.square(self.mu) * self.invcov).sum(1) est de dimension 64 \n",
    "# A est de dimension 64\n",
    "\n",
    "mu = numpy.load(\"mu.npy\")\n",
    "\n",
    "### Pure python implementation ###\n",
    "start = perf()\n",
    "a_idpt2 = []\n",
    "for row_invcov, row_mu in zip(invcov, mu):\n",
    "    a_idpt2.append(0)\n",
    "    for el_invcov, el_mu in zip(row_invcov, row_mu):\n",
    "        a_idpt2[-1] += el_mu * el_invcov * el_mu\n",
    "end = perf()\n",
    "naive_imple = end-start\n",
    "\n",
    "### Numpy implementation ###\n",
    "start = perf()\n",
    "a_idpt = numpy.sum( numpy.square(mu) * invcov, axis=1 )\n",
    "end = perf()\n",
    "numpy_imple = end-start\n",
    "\n",
    "print(f\"Naive implementation took {naive_imple/1000} microseconds\\n\"\n",
    "      f\"Numpy implementation took {numpy_imple/1000} microseconds\\n\"\n",
    "      f\"So, numpy here is {naive_imple/numpy_imple:.2f} times faster\")\n",
    "\n",
    "print(a_idpt - numpy.array(a_idpt2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation took 3950.0 microseconds\n",
      "Numpy implementation took 169.3 microseconds\n",
      "So, numpy here is 23.33 times faster\n",
      "[ 0.00000000e+00  7.10542736e-15  0.00000000e+00 -7.10542736e-15\n",
      "  0.00000000e+00  0.00000000e+00 -3.55271368e-15 -7.10542736e-15\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.06581410e-14  7.10542736e-15  0.00000000e+00 -7.10542736e-15\n",
      "  0.00000000e+00  0.00000000e+00 -7.10542736e-15 -3.55271368e-15\n",
      " -3.55271368e-15  1.06581410e-14 -7.10542736e-15 -7.10542736e-15\n",
      "  1.77635684e-15 -7.10542736e-15 -1.42108547e-14  1.42108547e-14\n",
      "  0.00000000e+00 -7.10542736e-15  0.00000000e+00  3.55271368e-15\n",
      "  0.00000000e+00  0.00000000e+00  1.06581410e-14  3.55271368e-15\n",
      "  0.00000000e+00  7.10542736e-15 -7.10542736e-15  0.00000000e+00\n",
      " -3.55271368e-15 -1.06581410e-14 -3.55271368e-15 -3.55271368e-15\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  7.10542736e-15\n",
      " -3.55271368e-15  0.00000000e+00  3.55271368e-15 -7.10542736e-15\n",
      " -3.55271368e-15  0.00000000e+00  3.55271368e-15  0.00000000e+00\n",
      " -3.55271368e-15 -3.55271368e-15  0.00000000e+00  0.00000000e+00\n",
      " -7.10542736e-15  3.55271368e-15  3.55271368e-15  0.00000000e+00]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pareil pour le terme $$ x^T\\Sigma_i^{-1}x = \\sum_{n=1}^{39}{x^2 \\sigma_n} $$",
   "id": "9f2006403365e1ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:36:58.565538Z",
     "start_time": "2024-09-12T19:36:54.865392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = numpy.load(\"features/features_1.npy\")\n",
    "\n",
    "# Voici ce que j'avais initialement fait comme implementation.\n",
    "# Je la garde ici juste pour référence.\n",
    "# Comme on peut le voir, j'utilisais à la fois des boucles et du numpy. Donc il y a un peu d'optimisation, mais on voit que ça manque un peu quand même.\n",
    "\n",
    "### My first implementation ###\n",
    "def f(x):\n",
    "    return numpy.sum( numpy.square(x) * invcov, axis=1 )\n",
    "def g(x):\n",
    "    return numpy.sum( x * invcov * mu, axis=1 )\n",
    "\n",
    "b_dpt1 = []\n",
    "\n",
    "for x in X:\n",
    "    b_dpt1.append(f(x)-2.0*g(x))\n",
    "    \n",
    "### Pure python implementation ###\n",
    "start = perf()\n",
    "b_dpt2 = []\n",
    "for cep in X:\n",
    "    temp = []\n",
    "    for row_invcov, row_mu in zip(invcov, mu):\n",
    "        temp.append(0)\n",
    "        for el_cep, el_invcov, el_mu in zip(cep, row_invcov, row_mu):\n",
    "            temp[-1] += el_cep ** 2 * el_invcov - 2 * ( el_cep * el_invcov * el_mu )\n",
    "    b_dpt2.append(temp)\n",
    "end = perf()\n",
    "naive_imple = end-start\n",
    "\n",
    "### Numpy implementation ###\n",
    "start = perf()\n",
    "b_dpt = numpy.dot(numpy.square(X), invcov.T) - 2.0 * numpy.dot(X, numpy.transpose(mu * invcov))\n",
    "end = perf()\n",
    "numpy_imple = end-start\n",
    "\n",
    "print(f\"Naive implementation took {naive_imple/1000} microseconds\\n\"\n",
    "      f\"Numpy implementation took {numpy_imple/1000} microseconds\\n\"\n",
    "      f\"So, numpy here is {naive_imple/numpy_imple:.2f} times faster\")\n",
    "\n",
    "print(b_dpt - numpy.array(b_dpt2))"
   ],
   "id": "d40a6e4564dcd392",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation took 3670475.1 microseconds\n",
      "Numpy implementation took 693.5 microseconds\n",
      "So, numpy here is 5292.68 times faster\n",
      "[[ 0.00000000e+00  7.10542736e-15 -2.13162821e-14 ... -7.10542736e-15\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.42108547e-14  7.10542736e-15  0.00000000e+00 ... -1.42108547e-14\n",
      "   1.42108547e-14  1.42108547e-14]\n",
      " [ 0.00000000e+00 -1.42108547e-14  0.00000000e+00 ...  0.00000000e+00\n",
      "  -7.10542736e-15  7.10542736e-15]\n",
      " ...\n",
      " [-1.42108547e-14 -7.10542736e-15  9.76996262e-15 ...  1.42108547e-14\n",
      "   7.10542736e-15  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.42108547e-14  1.42108547e-14 ...  0.00000000e+00\n",
      "  -2.84217094e-14  2.84217094e-14]\n",
      " [ 1.77635684e-14 -2.13162821e-14  1.42108547e-14 ... -2.84217094e-14\n",
      "   7.10542736e-15 -1.42108547e-14]]\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "a738780d-c5ce-4136-a2e3-9a9911029daa",
   "metadata": {},
   "source": [
    "## 1.2) Écrivez une fonction compute_lpp \n",
    "\n",
    "Cette méthode de la class **Mixture** va calculer les log posterior probabilités $$\\log{p_i(x)}$$ d'un ensemble de vecteurs sur ce mélange de Gaussienne à matrices de covariances diagonales\n",
    "\n",
    "Pour acumuler les statistiques, vous allez créer un objet mixture que vous appelerez **accum**.\n",
    "\n",
    "* Cette méthode prend en paramètres une matrice de coefficients cepstraux de dimension N x F où N est le nombre de vecteurs (variable selon les fichiers) et F est la dimension des vecteurs (39 dans notre cas)\n",
    "* En premier lieu, pensez à extraire des boucles tous les termes qui ne dépendent pas des données\n",
    "* Cette méthode renvoie les $$n_i$$ (donc un vecteur)\n",
    "\n",
    "Pour rappel: \n",
    "$$p_i(x) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\Sigma_i|^\\frac{1}{2}} e^{-(\\frac{1}{2}(x - \\mu)^T\\Sigma_i^{-1}(x-\\mu))}$$\n",
    "sachant:\n",
    "$$ -(\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i)) = -\\frac{1}{2} ( x^T\\Sigma_i^{-1}x + \\mu_i^T\\Sigma_i^{-1}\\mu_i - x^T\\Sigma_i^{-1}\\mu_i - \\mu_i^T\\Sigma_i^{-1}x ) $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "73558128-8817-4016-aa2d-f8559ad436ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T19:38:53.679559Z",
     "start_time": "2024-09-12T19:38:51.918443Z"
    }
   },
   "source": [
    "# L'idée ici c'est de réutiliser les briques qu'on vient de construire ci-dessus pour calculer lpp\n",
    "\n",
    "def compute_lpp_naive(X):\n",
    "    # Compute the determinant\n",
    "    det = []\n",
    "    for line in invcov:\n",
    "        det.append(1)\n",
    "        for val in line:\n",
    "            det[-1] *= val\n",
    "        det[-1] = 1 / det[-1]\n",
    "        \n",
    "    # Compute the constant term\n",
    "    cst = []\n",
    "    for line in det:\n",
    "        cst.append( 1.0 / ((2.0 * math.pi)**(D/2.0) * (math.sqrt(line))) )\n",
    "    \n",
    "    # Compute the independent term in the exponent:\n",
    "    a_idpt = numpy.sum( numpy.square(mu) * invcov, axis=1 )\n",
    "    \n",
    "    # Calcul des parties dépendantes des données:\n",
    "    b_dpt2 = []\n",
    "    for cep in X:\n",
    "        temp = []\n",
    "        for row_invcov, row_mu in zip(invcov, mu):\n",
    "            temp.append(0)\n",
    "            for el_cep, el_invcov, el_mu in zip(cep, row_invcov, row_mu):\n",
    "                temp[-1] += el_cep ** 2 * el_invcov - 2 * ( el_cep * el_invcov * el_mu )\n",
    "        b_dpt2.append(temp)\n",
    "    \n",
    "    # Compute the log posterior probability\n",
    "    lpp = []\n",
    "    for row_b in b_dpt:\n",
    "        temp = []\n",
    "        for el_cst, el_a, el_b in zip(cst, a_idpt, row_b):\n",
    "            temp.append( math.log(el_cst) - 0.5 * ( el_a + el_b ) )\n",
    "        lpp.append(temp)\n",
    "    \n",
    "    return lpp\n",
    "\n",
    "\n",
    "def compute_lpp_numpy(X):\n",
    "    # Compute the determinant\n",
    "    det = 1.0 / numpy.prod(invcov, axis=1)\n",
    "        \n",
    "    # Compute the constant term\n",
    "    cst = 1.0 / ( numpy.power(2.0 * numpy.pi, D/2.0) * numpy.sqrt( det ))\n",
    "    \n",
    "    # Compute the independent term in the exponent:\n",
    "    a_idpt = numpy.sum( numpy.square(mu) * invcov, axis=1 )\n",
    "    \n",
    "    # Calcul des parties dépendantes des données:\n",
    "    b_dpt = numpy.dot(numpy.square(X), invcov.T) - 2.0 * numpy.dot(X, numpy.transpose(mu * invcov))\n",
    "    \n",
    "    # Compute the log posterior probability and return it\n",
    "    \n",
    "    return numpy.log(cst) - 0.5 * (a_idpt + b_dpt)\n",
    "\n",
    "start = perf()\n",
    "compute_lpp_naive(X)\n",
    "end = perf()\n",
    "naive_imple = end-start\n",
    "\n",
    "start = perf()\n",
    "compute_lpp_numpy(X)\n",
    "end = perf()\n",
    "numpy_imple = end-start\n",
    "\n",
    "print(f\"Naive implementation took {naive_imple/1000} microseconds\\n\"\n",
    "      f\"Numpy implementation took {numpy_imple/1000} microseconds\\n\"\n",
    "      f\"So, numpy here is {naive_imple/numpy_imple:.2f} times faster\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation took 1741968.9 microseconds\n",
      "Numpy implementation took 1097.5 microseconds\n",
      "So, numpy here is 1587.22 times faster\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "4f61a954-76a6-4ecf-b6b1-61ee35c4d235",
   "metadata": {},
   "source": [
    "## 1.3) Utilisez la fonction **sum_log_probabilities**\n",
    "* Que fait cette fonction qui vous est donnée?\n",
    "* Pourquoi l'utilise-t'on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b29bd63-76a6-4290-81b2-f6ae7f73693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_log_probabilities(lp):\n",
    "    \"\"\"Sum log probabilities in a secure manner to avoid extreme values\n",
    "\n",
    "    :param lp: numpy array of log-probabilities to sum\n",
    "    \"\"\"\n",
    "    pp_max = numpy.max(lp, axis=1)\n",
    "    log_lk = pp_max + numpy.log(numpy.sum(numpy.exp((lp.transpose() - pp_max).T), axis=1))\n",
    "    ind = ~numpy.isfinite(pp_max)\n",
    "    if sum(ind) != 0:\n",
    "        log_lk[ind] = pp_max[ind]\n",
    "    pp = numpy.exp((lp.transpose() - log_lk).transpose())\n",
    "    llk = log_lk.sum()\n",
    "    return pp, llk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1ee10-f7d7-4be1-9e0a-762be74bb52d",
   "metadata": {},
   "source": [
    "## 1.4) Bouclez sur les fichiers de paramètres pour accumuler les statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c44723-f60f-4371-b18d-6f2d4c98dbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
