{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "celtic-whale",
   "metadata": {},
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/fr/thumb/e/ed/Logo_Universit%C3%A9_du_Maine.svg/1280px-Logo_Universit%C3%A9_du_Maine.svg.png' width=\"300\" height=\"500\">\n",
    "<br>\n",
    "<div style=\"border: solid 3px #000;\">\n",
    "    <h1 style=\"text-align: center; color:#000; font-family:Georgia; font-size:26px;\">Infrastructures pour l'IA</h1>\n",
    "    <p style='text-align: center;'>Master Informatique 1</p>\n",
    "    <p style='text-align: center;'>Anhony Larcher</p>\n",
    "</div>\n",
    "\n",
    "Cette session est organisée comme un challenge:\n",
    "* Vous optimiserez un code afin d'effectuer un calcul le plus rapidement possible\n",
    "* le résultat sera utilisée dans le TP numéro 6 pour faire du calcul parallélisé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f937f-8bd2-40e8-8ed5-1d35c51837b6",
   "metadata": {},
   "source": [
    "# Implémenter le calcul suivant et optimisez le au mieux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170d483",
   "metadata": {},
   "source": [
    "Il s'agit d'accumuler les statistiques d'ordre 0 et d'ordre 1 sur un mélange de Gaussiennes. Vous trouverez sur Umtice une archive zip contenant un objet Mixture et des paramètres acoustiques de type MFCC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3f254-3206-4924-985d-7ad095b801e7",
   "metadata": {},
   "source": [
    "Chaque distribution de la mixture a pour densité de probabilité:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446f09b-e1b0-48cf-be6f-c6ab532bd8ac",
   "metadata": {},
   "source": [
    "Et la densité de probabilité du mélange de Gaussienne est \n",
    "$$p(x|\\lambda) = \\Sigma_{i=1}^{M}w_ip_i(x)$$\n",
    "\n",
    "où $$\\Sigma_{i=1}^{M}w_i = 1$$\n",
    "\n",
    "et \n",
    "\n",
    "$$p_i(x) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\Sigma_i|^\\frac{1}{2}} e^{-(\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-weather",
   "metadata": {},
   "source": [
    "Nous souhaiton ici calculer pour chaque trame $x_t$\n",
    "\n",
    "$$P(i|x_t) = \\frac{w_ip_i(x_t)}{\\Sigma_{j=1}^M w_j p_j(x(t))}$$\n",
    "\n",
    "Pour ensuite calculer les statistiques d'ordre 0 pour chaque gaussienne $i$:\n",
    "\n",
    "$$n_i = \\Sigma_{t=1}^{T}P(i|x_t)$$\n",
    "\n",
    "Les statistiques d'ordre 1:\n",
    "\n",
    "$$E_i(x) = \\frac{1}{n_i} \\Sigma_{t=1}^{T}P(i|x_t)x_t$$\n",
    "\n",
    "Et les statistiques d'ordre 2:\n",
    "\n",
    "$$E_i(x^2) = \\frac{1}{n_i} \\Sigma_{t=1}^{T}P(i|x_t)x_t^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54c6e9",
   "metadata": {},
   "source": [
    "## 1.1) Initialisez l'accumulateur de statistiques\n",
    "\n",
    "Le fichier ``mixture.py``contient le code d'une classe Mixtuyre qui sera utilisé dans ce TP. Ce code se trouve également dans la cellule ci-dessous pour travailler dans le notebook. Une fois la cellule ci-dessous complétée vous pourrez recopier le code dans le fichier ``mixture.py``afin d'avoir une version propre du code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47f2971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Mixture(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize an empty Mixture object\n",
    "        \"\"\"\n",
    "        self.w = numpy.array([])\n",
    "        self.mu = numpy.array([])\n",
    "        self.invcov = numpy.array([])\n",
    "        self.cst = numpy.array([])\n",
    "        self.det = numpy.array([])\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Serialize a Mixture object to text\n",
    "        \"\"\"\n",
    "        return f'w = {self.w}\\nmu =  {selmf.mu}\\ninvcov = {self.invcov}, cst = {self.cst}\\ndet = {self.det}'\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, filename):\n",
    "        \"\"\"\n",
    "        Read a Mixture object stored in Pickle format on disk\n",
    "        :param filename: the name of the file to read from\n",
    "        :return: a Mixture object\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as fh:\n",
    "            mixture = pickle.load(fh)\n",
    "            return mixture\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Save a Mixture object to disk in Pickle format\n",
    "        :param filename: the name of the file to write to\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as fh:\n",
    "            pickle.dump(self, fh)\n",
    "\n",
    "    def compute_lpp_1(self, cep, mu=None):\n",
    "        \"\"\" Compute log posterior probabilities for a set of feature frames.\n",
    "\n",
    "        :param cep: a set of feature frames in a ndarray, one feature per row\n",
    "        :param mu: a mean super-vector to replace the ubm's one. If it is an empty \n",
    "              vector, use the UBM, dimension is: \n",
    "\n",
    "        :return: A ndarray of log-posterior probabilities corresponding to the \n",
    "              input feature set.\n",
    "        \"\"\"\n",
    "        mu = self.mu\n",
    "        # Get the dimension of the feature vector from the cep input\n",
    "        self.dim = self.mu.shape[1]\n",
    "\n",
    "        # Compute the value of the determinant from the covariance matrix\n",
    "        self.det = numpy.zeros(64)\n",
    "        for ii in range(self.dim):\n",
    "            self.det[ii] = 1.0 / numpy.prod(self.invcov[ii, :])\n",
    "\n",
    "        # for each Guassian distribution, compute the term that is invariant\n",
    "        self.cst = numpy.zeros(64)\n",
    "        for ii in range(self.dim):\n",
    "            self.cst[ii] = 1.0 / (numpy.sqrt(self.det[ii]) * (2.0 * numpy.pi) ** (self.dim / 2.0))\n",
    "\n",
    "        A = numpy.zeros(64)\n",
    "        # For each feature, compute the data independent term\n",
    "        for ii in range(self.dim):\n",
    "            A[ii] = (numpy.square(self.mu[ii, :]) * self.invcov[ii, :]).sum() - 2.0 * (numpy.log(self.w[ii]) + numpy.log(self.cst[ii]))\n",
    "\n",
    "        # Compute the data dependent term\n",
    "        # numpy.square(cep) est de dimension Nx39\n",
    "        # numpy.dot(numpy.square(cep), self.invcov.T) est de dimension Nx39 multiplié par 39x64 -> dimension Nx64\n",
    "        # mu.reshape(self.mu.shape) * self.invcov) est de dimension 64x39 \n",
    "        # numpy.transpose(mu.reshape(self.mu.shape) * self.invcov) est de dimension 39x64\n",
    "        # numpy.dot(cep, numpy.transpose(mu.reshape(self.mu.shape) * self.invcov)) est de dimension Nx39 multiplié par 39x64 -> Nx64\n",
    "        # B est de dimension Nx64\n",
    "\n",
    "        B = numpy.zeros((cep.shape[0], 64))\n",
    "        # for each feature and each distribution\n",
    "        for tt in range(cep.shape[0]):\n",
    "            for ii in range(self.dim):\n",
    "                B[tt, ii] = numpy.dot(numpy.square(cep[tt, :]), self.invcov[ii, :].T) - 2.0 * numpy.dot(cep[tt, :], numpy.transpose(mu[ii, :].reshape(self.dim) * self.invcov[ii, :]))\n",
    "\n",
    "\n",
    "        # Compute the exponential term\n",
    "        lp = -0.5 * (B + A)\n",
    "        #for tt in range(cep.shape[0]):\n",
    "        #    for ii in range(self.dim):\n",
    "        #        pass\n",
    "        return lp\n",
    "\n",
    "    def compute_log_posterior_probabilities(self, cep, mu=None):\n",
    "        \"\"\" Compute log posterior probabilities for a set of feature frames.\n",
    "\n",
    "        :param cep: a set of feature frames in a ndarray, one feature per row\n",
    "        :param mu: a mean super-vector to replace the ubm's one. If it is an empty \n",
    "              vector, use the UBM, dimension is: \n",
    "\n",
    "        :return: A ndarray of log-posterior probabilities corresponding to the \n",
    "              input feature set.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dim = self.mu.shape[1]\n",
    "        self.det = 1.0 / numpy.prod(self.invcov, axis=1)      # self.det est de dimension 64\n",
    "        self.cst = 1.0 / (numpy.sqrt(self.det) * (2.0 * numpy.pi) ** (self.dim / 2.0))    # cst est de dimension 64\n",
    "\n",
    "        mu = self.mu\n",
    "        # for MAP, Compute the data independent term\n",
    "        # self.mu est de dimension 64x39\n",
    "        # self.invcov est de dimension 64x39\n",
    "        # numpy.square(self.mu) * self.invcov) est de dimension 64x39\n",
    "        # numpy.square(self.mu) * self.invcov).sum(1) est de dimension 64 \n",
    "        # A est de dimension 64\n",
    "        A = (numpy.square(self.mu) * self.invcov).sum(1) - 2.0 * (numpy.log(self.w) + numpy.log(self.cst))\n",
    "\n",
    "\n",
    "        # Compute the data dependent term\n",
    "        #  numpy.square(cep) est de dimension Nx39\n",
    "        # numpy.dot(numpy.square(cep), self.invcov.T) est de dimension Nx39 multiplié par 39x64 -> dimension Nx64\n",
    "        # mu.reshape(self.mu.shape) * self.invcov) est de dimension 64x39 \n",
    "        # numpy.transpose(mu.reshape(self.mu.shape) * self.invcov) est de dimension 39x64\n",
    "        # numpy.dot(cep, numpy.transpose(mu.reshape(self.mu.shape) * self.invcov)) est de dimension Nx39 multiplié par 39x64 -> Nx64\n",
    "        # B est de dimension Nx64\n",
    "        B = numpy.dot(numpy.square(cep), self.invcov.T) - 2.0 * numpy.dot(cep, numpy.transpose(mu.reshape(self.mu.shape) * self.invcov))\n",
    "\n",
    "        # Compute the exponential term\n",
    "        lp = -0.5 * (B + A)\n",
    "        return lp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7770b207-f7ee-458a-b202-4ec427aec9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338, 39)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "data = numpy.load(\"data_1/features_1.npy\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a370e",
   "metadata": {},
   "source": [
    "## Analyse des formules\n",
    "\n",
    "On remarque d'abord que certains éléments dépendent des données, et plus particulièrement de chaque vecteur de donnée, mais également de chaque distribution de la mixture.\n",
    "\n",
    "Nous verrons donc apparaitre 2 boucles principales:\n",
    "* une sur les données\n",
    "* une sur les distributions\n",
    "\n",
    "Pour simplifier le calcul nous devons donc séparer ce qui est indépendant de ces boucles afin de ne pas le recalculer plusieurs fois.\n",
    "\n",
    "Ré-écrivez $$p_i(x)$$ en séparant ces termes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c5321",
   "metadata": {},
   "source": [
    "$$p_i(x) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\Sigma_i|^\\frac{1}{2}} e^{-(\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i))}$$\n",
    "\n",
    "On trouve d'abord un terme qui dépend de chaque mixture: leur déterminant"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f91e00b",
   "metadata": {},
   "source": [
    "# Il faut donc calculer ce déterminant une seule fois:\n",
    "# Le déterminant d'une matrice diagonale est le produit de ses termes diagonaux.\n",
    "# Les covariances inverses étant stockées par ligne dans self.invcov nous calculons\n",
    "\n",
    "self.det = numpy.zeros(self.dim)\n",
    "self.det = 1.0 / numpy.prod(self.invcov, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b657683",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant calculer le premier terme pour chaque distribution:\n",
    "    \n",
    "$$ \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\Sigma_i|^\\frac{1}{2}} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267387b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pour chaque distribution, le terme invariant est:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mdim):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcst[ii] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (numpy\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdet[ii]) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m numpy\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Pour chaque distribution, le terme invariant est:\n",
    "\n",
    "for ii in range(self.dim):\n",
    "    self.cst[ii] = 1.0 / (numpy.sqrt(self.det[ii]) * (2.0 * numpy.pi) ** (self.dim / 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943af79",
   "metadata": {},
   "source": [
    "Regardons maintenant le terme contenu dans l'exponentielle:\n",
    "\n",
    "$$ -(\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i)) $$ \n",
    "\n",
    "Tout ce terme dépend des distributions, une partie seulement dépend des données.\n",
    "\n",
    "$$ -(\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i)) = -\\frac{1}{2} ( x^T\\Sigma_i^{-1}x + \\mu_i^T\\Sigma_i^{-1}\\mu_i - x^T\\Sigma_i^{-1}\\mu_i - \\mu_i^T\\Sigma_i^{-1}x ) $$ \n",
    "\n",
    "Calculez chacun de ces termes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac07f93",
   "metadata": {},
   "source": [
    "Analyse du terme independant des données:\n",
    "\n",
    "$$ \\mu_i^T\\Sigma_i^{-1}\\mu_i $$\n",
    "\n",
    "$$ \\mu_i^T$$ est de dimension (1, 39) $$\\Sigma_i$$ est de dimension (39, 39)\n",
    "\n",
    "Donc $$ \\mu_i^T\\Sigma_i^{-1}\\mu_i $$ est de dimension (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En pratique, comme la matruice de covariance est diagonale, nous n'avons pas besoin de calculer un vrai produit matriciel\n",
    "# Écrivez la valeur du premier terme de \\mu_i \\Sigma^{-1}_i et tirez partie de cette expression pour simplifier\n",
    "# le calcul de ce terme\n",
    "\n",
    "# self.mu est de dimension 64x39\n",
    "# self.invcov est de dimension 64x39\n",
    "# numpy.square(self.mu) * self.invcov) est de dimension 64x39\n",
    "# numpy.square(self.mu) * self.invcov).sum(1) est de dimension 64 \n",
    "# A est de dimension 64\n",
    "\n",
    "for ii in range(self.dim):\n",
    "    A_idpt[ii] = (numpy.square(self.mu[ii, :]) * self.invcov[ii, :]).sum(1)\n",
    "\n",
    "# Allez plus loin en vectorisant entre toutes les distributions\n",
    "A_idpt = (numpy.square(self.mu) * self.invcov).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e1236",
   "metadata": {},
   "source": [
    "De la même façon calculez les termes dépendant des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9c187b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-8307b2353daf>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    - 2.0 * numpy.dot(cep, numpy.transpose(self.mu.reshape(self.mu.shape) * self.invcov))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "B = numpy.dot(numpy.square(cep), self.invcov.T) \n",
    "    - 2.0 * numpy.dot(cep, numpy.transpose(self.mu.reshape(self.mu.shape) * self.invcov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714ce722",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'features_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m ubm\u001b[38;5;241m.\u001b[39minvcov \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvcov.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures_0.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m data\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     10\u001b[0m lp \u001b[38;5;241m=\u001b[39m ubm\u001b[38;5;241m.\u001b[39mcompute_lpp_1(data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:417\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    418\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features_0.npy'"
     ]
    }
   ],
   "source": [
    "ubm = Mixture()\n",
    "ubm.w = numpy.load('w.npy')\n",
    "ubm.mu = numpy.load('mu.npy')\n",
    "ubm.invcov = numpy.load('invcov.npy')\n",
    "\n",
    "# load data\n",
    "data = numpy.load('features_0.npy')\n",
    "data.shape\n",
    "\n",
    "lp = ubm.compute_lpp_1(data)\n",
    "lp2 = ubm.compute_log_posterior_probabilities(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738780d-c5ce-4136-a2e3-9a9911029daa",
   "metadata": {},
   "source": [
    "## 1.2) Écrivez une fonction compute_lpp \n",
    "\n",
    "Cette méthode de la class **Mixture** va calculer les log posterior probabilités $$\\log{p_i(x)}$$ d'un ensemble de vecteurs sur ce mélange de Gaussienne à matrices de covariances diagonales\n",
    "\n",
    "Pour acumuler les statistiques, vous allez créer un objet mixture que vous appelerez **accum**.\n",
    "\n",
    "* Cette méthode prend en paramètres une matrice de coefficients cepstraux de dimension N x F où N est le nombre de vecteurs (variable selon les fichiers) et F est la dimension des vecteurs (39 dans notre cas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3fe3c",
   "metadata": {},
   "source": [
    "### Première version\n",
    "\n",
    "Dans un premier temps écrivez une version immédiate des équations ci-dessus afin d'avoir un premier code, facile à lire et qui permettra par la suite de vérifier l'exactitude du résultat et de comparer l'eficacité des différentes versions du code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff538574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lpp_1(self, cep, mu=None):\n",
    "    \"\"\" Compute log posterior probabilities for a set of feature frames.\n",
    "\n",
    "    :param cep: a set of feature frames in a ndarray, one feature per row\n",
    "    :param mu: a mean super-vector to replace the ubm's one. If it is an empty \n",
    "          vector, use the UBM, dimension is: \n",
    "\n",
    "    :return: A ndarray of log-posterior probabilities corresponding to the \n",
    "          input feature set.\n",
    "    \"\"\"\n",
    "    # Get the dimension of the feature vector from the cep input\n",
    "    self.dim = self.mu.shape[1]\n",
    "    \n",
    "    # Compute the value of the determinant from the covariance matrix\n",
    "    for ii in range(self.dim):\n",
    "        self.det[ii] = 1.0 / numpy.prod(self.invcov[ii, :])\n",
    "    \n",
    "    # for each Guassian distribution, compute the term that is invariant\n",
    "    for ii in range(self.dim):\n",
    "        self.cst[ii] = 1.0 / (numpy.sqrt(self.det[ii]) * (2.0 * numpy.pi) ** (self.dim / 2.0))\n",
    "    \n",
    "    # For each feature, compute the data independent term\n",
    "    for ii in range(self.dim):\n",
    "        A[ii] = (numpy.square(self.mu[ii, :]) * self.invcov[ii, :]).sum() - 2.0 * (numpy.log(self.w[ii]) + numpy.log(self.cst[ii]))\n",
    "    \n",
    "    # Compute the data dependent term\n",
    "    # numpy.square(cep) est de dimension Nx39\n",
    "    # numpy.dot(numpy.square(cep), self.invcov.T) est de dimension Nx39 multiplié par 39x64 -> dimension Nx64\n",
    "    # mu.reshape(self.mu.shape) * self.invcov) est de dimension 64x39 \n",
    "    # numpy.transpose(mu.reshape(self.mu.shape) * self.invcov) est de dimension 39x64\n",
    "    # numpy.dot(cep, numpy.transpose(mu.reshape(self.mu.shape) * self.invcov)) est de dimension Nx39 multiplié par 39x64 -> Nx64\n",
    "    # B est de dimension Nx64\n",
    "    \n",
    "    # for each feature and each distribution\n",
    "    for tt in range(cep.shape[0]):\n",
    "        for ii in range(self.dim):\n",
    "            B[tt, ii] = numpy.dot(numpy.square(cep[tt, :]), self.invcov[ii, :].T) - 2.0 * numpy.dot(cep[tt, :], numpy.transpose(mu[tt, :].reshape(self.dim) * self.invcov[ii, :]))\n",
    "\n",
    "\n",
    "    # Compute the exponential term\n",
    "    #lp = -0.5 * (B + A)\n",
    "    for tt in range(cep.shape[0]):\n",
    "        for ii in range(self.dim):\n",
    "            pass\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ac973",
   "metadata": {},
   "source": [
    "* En premier lieu, pensez à extraire des boucles tous les termes qui ne dépendent pas des données\n",
    "* Cette méthode renvoie les $$n_i$$ (donc un vecteur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417b109-b1fe-468b-a983-b8013e78e946",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "* En premier lieu, cette méthode calcule le terme de l'équation qui est indépendant des données.\n",
    "\n",
    "$$\\Sigma(\\mu^2*\\Sigma^{-1}) - 2.*\\log{w} + \\log{cst}$$\n",
    "\n",
    "Le calcul de $$\\mu^T\\Sigma^{-1}\\mu$$ peut déjà être optimisé car la covariance est diagonale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca860ce8-4e75-41b1-9a95-995c31af8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_posterior_probabilities(self, cep, mu=None):\n",
    "    \"\"\" Compute log posterior probabilities for a set of feature frames.\n",
    "\n",
    "    :param cep: a set of feature frames in a ndarray, one feature per row\n",
    "    :param mu: a mean super-vector to replace the ubm's one. If it is an empty \n",
    "          vector, use the UBM, dimension is: \n",
    "\n",
    "    :return: A ndarray of log-posterior probabilities corresponding to the \n",
    "          input feature set.\n",
    "    \"\"\"\n",
    "\n",
    "    self.dim = self.mu.shape[1]\n",
    "    self.det = 1.0 / numpy.prod(self.invcov, axis=1)      # self.det est de dimension 64\n",
    "    self.cst = 1.0 / (numpy.sqrt(self.det) * (2.0 * numpy.pi) ** (self.dim / 2.0))    # cst est de dimension 64\n",
    "    \n",
    "    # for MAP, Compute the data independent term\n",
    "    # self.mu est de dimension 64x39\n",
    "    # self.invcov est de dimension 64x39\n",
    "    # numpy.square(self.mu) * self.invcov) est de dimension 64x39\n",
    "    # numpy.square(self.mu) * self.invcov).sum(1) est de dimension 64 \n",
    "    # A est de dimension 64\n",
    "    A = (numpy.square(self.mu) * self.invcov).sum(1) - 2.0 * (numpy.log(self.w) + numpy.log(self.cst))\n",
    "\n",
    "\n",
    "    # Compute the data dependent term\n",
    "    #  numpy.square(cep) est de dimension Nx39\n",
    "    # numpy.dot(numpy.square(cep), self.invcov.T) est de dimension Nx39 multiplié par 39x64 -> dimension Nx64\n",
    "    # mu.reshape(self.mu.shape) * self.invcov) est de dimension 64x39 \n",
    "    # numpy.transpose(mu.reshape(self.mu.shape) * self.invcov) est de dimension 39x64\n",
    "    # numpy.dot(cep, numpy.transpose(mu.reshape(self.mu.shape) * self.invcov)) est de dimension Nx39 multiplié par 39x64 -> Nx64\n",
    "    # B est de dimension Nx64\n",
    "    B = numpy.dot(numpy.square(cep), self.invcov.T) - 2.0 * numpy.dot(cep, numpy.transpose(mu.reshape(self.mu.shape) * self.invcov))\n",
    "\n",
    "    # Compute the exponential term\n",
    "    lp = -0.5 * (B + A)\n",
    "    return lp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61a954-76a6-4ecf-b6b1-61ee35c4d235",
   "metadata": {},
   "source": [
    "## 1.3) Utilisez la fonction **sum_lp**\n",
    "* Que fait cette fonction qui vous est donnée?\n",
    "* Pourquoi l'utilise-t'on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cefa5b-52ad-42ff-8ee9-6b2efe7b1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_log_probabilities(lp):\n",
    "    \"\"\"Sum log probabilities in a secure manner to avoid extreme values\n",
    "\n",
    "    :param lp: numpy array of log-probabilities to sum\n",
    "    \"\"\"\n",
    "    pp_max = numpy.max(lp, axis=1)\n",
    "    log_lk = pp_max + numpy.log(numpy.sum(numpy.exp((lp.transpose() - pp_max).T), axis=1))\n",
    "    ind = ~numpy.isfinite(pp_max)\n",
    "    if sum(ind) != 0:\n",
    "        log_lk[ind] = pp_max[ind]\n",
    "    pp = numpy.exp((lp.transpose() - log_lk).transpose())\n",
    "    llk = log_lk.sum()\n",
    "    return pp, llk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd2d12-ec87-454e-bbc6-1aff654bb01c",
   "metadata": {},
   "source": [
    "# Mesure du temps de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34187940-bc08-44d0-9d92-7d36b74b7ee6",
   "metadata": {},
   "source": [
    "## 1.4) Bouclez sur les fichiers de paramètres pour accumuler les statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0c44723-f60f-4371-b18d-6f2d4c98dbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 1:  1.457082986831665\n",
      "Elapsed time 2:  0.0023429393768310547\n",
      "Gain = 621.9038363691869\n"
     ]
    }
   ],
   "source": [
    "ubm = Mixture()\n",
    "ubm.w = numpy.load('w.npy')\n",
    "ubm.mu = numpy.load('mu.npy')\n",
    "ubm.invcov = numpy.load('invcov.npy')\n",
    "\n",
    "accum = Mixture()\n",
    "accum.w = numpy.zeros(ubm.w.shape)\n",
    "accum.cst = numpy.zeros(ubm.w.shape)\n",
    "accum.det = numpy.zeros(ubm.w.shape)\n",
    "accum.w = numpy.zeros(ubm.w.shape)\n",
    "accum.mu = numpy.zeros(ubm.mu.shape)\n",
    "accum.invcov = numpy.zeros(ubm.invcov.shape)\n",
    "\n",
    "#for ii in range(100):\n",
    "ii = 0\n",
    "data = numpy.load(f'data_1/features_{ii}.npy')\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "lp = ubm.compute_lpp_1(data)\n",
    "end_time = time.time()\n",
    "elapsed_time_1 = end_time - start_time\n",
    "print(\"Elapsed time 1: \", elapsed_time_1) \n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "lp = ubm.compute_log_posterior_probabilities(data)\n",
    "end_time = time.time()\n",
    "elapsed_time_2 = end_time - start_time\n",
    "print(\"Elapsed time 2: \", elapsed_time_2) \n",
    "\n",
    "print(f'Gain = {elapsed_time_1 / elapsed_time_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43068655-f698-4453-8e82-1b863c10484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "# zero order statistics\n",
    "accum.w += pp.sum(0)\n",
    "# first order statistics\n",
    "accum.mu += numpy.dot(cep.T, pp).T\n",
    "# second order statistics\n",
    "accum.invcov += numpy.dot(numpy.square(cep.T), pp).T  # version for diagonal covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87d6af28-b780-471b-be58-340e928a5438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57644626, 0.68576746, 0.62941113, ..., 0.75738399, 0.57559068,\n",
       "        0.66871915],\n",
       "       [0.80763037, 0.76329883, 0.69521484, ..., 0.87043643, 0.69031538,\n",
       "        0.74854336],\n",
       "       [0.81601115, 0.76350196, 0.72462563, ..., 0.81519951, 0.81604256,\n",
       "        0.86340996],\n",
       "       ...,\n",
       "       [0.72634523, 0.59532771, 0.71438645, ..., 0.72438607, 0.7639669 ,\n",
       "        0.77606345],\n",
       "       [0.71545848, 0.84455897, 0.84399906, ..., 0.63912259, 0.78547948,\n",
       "        0.91511047],\n",
       "       [0.59150909, 0.61340457, 0.68649622, ..., 0.61529548, 0.71058816,\n",
       "        0.57820938]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubm.invcov = numpy.load('invcov.npy')\n",
    "ubm.invcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3510980-21dd-4701-9ae4-2abc5234c8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
